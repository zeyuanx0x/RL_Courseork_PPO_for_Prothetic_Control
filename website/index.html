<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RL PPO Coursework Results</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <header class="header">
        <div class="header-background">
            <img src="RL_prothetic.png" alt="RL Prosthetic Background" class="header-image">
        </div>
        <div class="container header-content">
            <h1 class="logo">RL PPO Coursework</h1>
            <p class="subtitle">Full Results Visualization of PPO on Myoeletrical elbow task</p>
        </div>
    </header>

    <nav class="navbar">
        <div class="container">
            <ul class="nav-links">
                <li><a href="#training">Training</a></li>
                <li><a href="#performance">Performance</a></li>
                <li><a href="#algorithm">Algorithm</a></li>
                <li><a href="#collaboration">Collaboration</a></li>
                <li><a href="#perturbation">Perturbation</a></li>
                <li><a href="#safety">Safety</a></li>
                <li><a href="#generalization">Generalization</a></li>
                <li><a href="#failure">Failure</a></li>
                <li><a href="#value">Value Function</a></li>
            </ul>
        </div>
    </nav>

    <main class="main">
        <div class="container">
            <!-- Introduction -->
            <section class="intro">
                <p>This website presents a complete collection of figures generated from a systematic evaluation of reinforcement learning policies in a musculoskeletal control environment. All visualizations are produced from post-training data analysis and are accompanied by concise explanations that describe what each figure shows and why it matters for understanding policy behavior, stability, safety, and generalization.</p>
                <p>The results are intended to provide transparency into the learning dynamics and control characteristics of the studied methods, beyond standard reward curves. They may be useful for researchers and students interested in reinforcement learning, biomechanical simulation, and human–machine control.</p>
                <p>For access to high-resolution versions of the figures, please contact izer0x@outlook.com.</p>
            </section>

            <!-- Training and Learning Curves -->
            <section id="training" class="section">
                <h2 class="section-title">Training and Learning Curves</h2>
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/learning_curve_myoElbowPose1D6MRandom-v0.png" alt="Learning Curve" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Learning Curve</h3>
                            <p class="figure-description">Learning curve comparison between PPO and SAC. The episode returns of Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) are plotted over 100 training episodes. PPO consistently achieves higher episode returns with relatively smaller fluctuations compared to SAC, indicating more stable learning performance under the same training setting. PPO demonstrates superior and more stable performance than SAC throughout training, as reflected by consistently higher episode returns across episodes.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/ppo_training_curve_myoElbowPose1D6MRandom-v0.png" alt="PPO Training Curve" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">PPO Training Curve</h3>
                            <p class="figure-description">PPO training curves on the myoElbowPose1D6MRandom-v0 environment. (a) Episode returns and (b) episode lengths across training episodes illustrate stable task performance. (c) KL divergence and (d) clip fraction indicate controlled policy updates under the PPO clipping mechanism. (e) Policy entropy reflects a balanced exploration–exploitation trade-off, while (f) training losses (total, policy, and value) remain stable throughout training. Overall, these metrics suggest stable and well-regularized PPO training dynamics.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/seed_comparison_myoElbowPose1D6MRandom-v0.png" alt="Seed Comparison" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Seed Comparison</h3>
                            <p class="figure-description">PPO performance across random seeds. Mean episode returns obtained from five different random seeds are shown. The dashed line indicates the overall mean performance. Results demonstrate consistent performance across seeds, suggesting robustness of PPO training under random initialization.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Performance Analysis -->
            <section id="performance" class="section">
                <h2 class="section-title">Performance Analysis</h2>
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/episode_length_vs_return_myoElbowPose1D6MRandom-v0.png" alt="Episode Length vs Return" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Episode Length vs Return</h3>
                            <p class="figure-description">Relationship between episode length and episode return for PPO and SAC. Each point represents a single episode. PPO achieves higher and more consistent returns within a narrower range of episode lengths, while SAC exhibits greater variability in both episode length and return. No clear positive correlation between episode length and return is observed for either algorithm. These results suggest that PPO learns more efficient policies that achieve higher returns without requiring longer episodes.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/return_distribution_myoElbowPose1D6MRandom-v0.png" alt="Return Distribution" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Return Distribution</h3>
                            <p class="figure-description">Distribution of episode returns for PPO and SAC. Box plots summarize the distribution of episode returns across evaluation episodes. PPO demonstrates a higher median return and reduced variability compared to SAC, indicating more stable and consistent performance. PPO exhibits a higher central tendency and narrower interquartile range, suggesting improved stability relative to SAC.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Algorithm Comparison -->
            <section id="algorithm" class="section">
                <h2 class="section-title">Algorithm Comparison</h2>
                <!-- Comprehensive Multi-Algorithm Comparison -->
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/comprehensive_multi_algorithm_comparison_myoElbowPose1D6MRandom-v0.png" alt="Comprehensive Multi-Algorithm Comparison" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Comprehensive Multi-Algorithm Comparison</h3>
                            <p class="figure-description">PPO substantially outperforms heuristic and non-adaptive baselines across all evaluated metrics, demonstrating its ability to learn robust and task-relevant control policies for muscle-driven prosthetic systems.</p>
                        </div>
                    </div>
                </div>
                
                <div class="custom-layout">
                    <!-- Left: Combined Statistical Comparison (vertical) -->
                    <div class="figure-card vertical-card">
                        <div class="figure-image-container vertical-image">
                            <img src="figures/combined_statistical_comparison_myoElbowPose1D6MRandom-v0.png" alt="Combined Statistical Comparison" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Combined Statistical Comparison</h3>
                            <p class="figure-description">Comprehensive statistical comparison between PPO and SAC across multiple evaluation metrics. Bars represent mean values with error bars indicating variability. Statistical significance is denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001). PPO achieves higher returns, robustness, safety, and success rates, while significantly reducing both human and exoskeleton energy consumption.</p>
                        </div>
                    </div>

                    <!-- Right: Two horizontal cards -->
                    <div class="right-column">
                        <div class="figure-card">
                            <div class="figure-image-container">
                                <img src="figures/statistical_comparison_myoElbowPose1D6MRandom-v0.png" alt="Statistical Comparison" class="figure-image">
                            </div>
                            <div class="figure-info">
                                <h3 class="figure-title">Statistical Comparison</h3>
                                <p class="figure-description">Statistical comparison of episode returns between PPO and SAC. Markers indicate mean episode returns, with error bars representing variability across evaluation episodes. PPO achieves a higher average return compared to SAC.</p>
                            </div>
                        </div>

                        <div class="figure-card">
                            <div class="figure-image-container">
                                <img src="figures/statistical_effect_sizes_myoElbowPose1D6MRandom-v0.png" alt="Statistical Effect Sizes" class="figure-image">
                            </div>
                            <div class="figure-info">
                                <h3 class="figure-title">Statistical Effect Sizes</h3>
                                <p class="figure-description">Effect sizes (Cliff’s Delta) comparing PPO and SAC across evaluation metrics. Positive values indicate higher performance of PPO relative to SAC, while negative values indicate lower energy consumption. PPO demonstrates large effect sizes in returns and robustness, as well as substantial reductions in human and exoskeleton energy usage.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Human-Machine Collaboration Analysis -->
            <section id="collaboration" class="section">
                <h2 class="section-title">Human-Machine Collaboration Analysis</h2>
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/credit_human_vs_exo_load_myoElbowPose1D6MRandom-v0.png" alt="Human vs Exoskeleton Load Scatter Plot" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Human vs Exoskeleton Load Scatter Plot</h3>
                            <p class="figure-description">Human versus exoskeleton load distribution under different control modes. Scatter plots illustrate the relationship between human and exoskeleton load contributions. Human-dominant, exoskeleton-dominant, and shared control modes exhibit distinct load allocation patterns, with the shared mode demonstrating balanced load sharing between human and exoskeleton. The shared control mode enables a balanced distribution of load between the human and the exoskeleton, rather than shifting the burden entirely to a single agent.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/credit_load_distribution_myoElbowPose1D6MRandom-v0.png" alt="Load Distribution" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Load Distribution</h3>
                            <p class="figure-description">Average load ratio across different control modes. Stacked bar charts summarize the relative load contributions of the human and the exoskeleton. The shared control mode achieves a more balanced load distribution compared to human-dominant and exoskeleton-dominant modes. On average, shared control results in approximately equal load contributions from the human and the exoskeleton.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/credit_load_efficiency_myoElbowPose1D6MRandom-v0.png" alt="Load Efficiency" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Load Efficiency</h3>
                            <p class="figure-description">Load efficiency under different control modes. Box plots show the distribution of return per unit load for human-only, exoskeleton-only, and shared control modes. The shared mode achieves the highest load efficiency, indicating more effective utilization of combined human–exoskeleton effort. Shared control not only redistributes load but also improves the efficiency with which load is converted into task performance.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/credit_return_comparison_myoElbowPose1D6MRandom-v0.png" alt="Human-Machine Return Comparison" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Human-Machine Return Comparison</h3>
                            <p class="figure-description">Comparison of episode returns under different control modes. Box plots illustrate the distribution of episode returns achieved with human-only, exoskeleton-only, and shared control. The shared control mode consistently attains higher returns and a higher median performance compared to single-agent control modes. Shared control yields substantially higher episode returns than either human-only or exoskeleton-only control, indicating superior task performance.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Perturbation Analysis -->
            <section id="perturbation" class="section">
                <h2 class="section-title">Perturbation Analysis</h2>
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/perturbation_delay_myoElbowPose1D6MRandom-v0.png" alt="Delay Perturbation Impact" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Delay Perturbation Impact</h3>
                            <p class="figure-description">This line chart shows how performance (return) changes under different delay perturbation intensities. It demonstrates the system's robustness to communication delays, with performance degradation occurring gradually as delay increases.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/perturbation_force_myoElbowPose1D6MRandom-v0.png" alt="External Force Perturbation Impact" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">External Force Perturbation Impact</h3>
                            <p class="figure-description">This plot displays the effect of external force perturbations on agent performance. It shows performance metrics across different force magnitudes, revealing how the system responds to external disturbances.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/perturbation_noise_myoElbowPose1D6MRandom-v0.png" alt="Noise Perturbation Impact" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Noise Perturbation Impact</h3>
                            <p class="figure-description">This figure illustrates the impact of sensor noise on system performance. It plots performance metrics against different noise levels, demonstrating the agent's ability to handle noisy observations.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Safety and Physiological Analysis -->
            <section id="safety" class="section">
                <h2 class="section-title">Safety and Physiological Analysis</h2>
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/physio_smoothness_myoElbowPose1D6MRandom-v0.png" alt="Smoothness Metrics" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Smoothness Metrics</h3>
                            <p class="figure-description">Action smoothness across episodes. Action smoothness, measured as the average absolute change between consecutive actions, remains within a stable range throughout training, indicating smooth and well-behaved control signals. The learned policy generates smooth control actions without exhibiting abrupt oscillations.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/physio_force_proxy_myoElbowPose1D6MRandom-v0.png" alt="Force Proxy Analysis" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Force Proxy Analysis</h3>
                            <p class="figure-description">Action force proxy over episodes. The squared action magnitude, used as a proxy for control effort, remains bounded across episodes, suggesting moderate and consistent actuation demands. Control effort remains bounded, supporting the feasibility of the learned policy for physical human–exoskeleton interaction.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/physio_jerk_myoElbowPose1D6MRandom-v0.png" alt="Jerk Analysis" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Jerk Analysis</h3>
                            <p class="figure-description">Action jerk across episodes. Action jerk, quantified as the second-order difference of actions, remains low and stable, indicating the absence of abrupt or impulsive control behavior. Low action jerk further suggests that the learned policy produces physiologically plausible and mechanically safe motion profiles.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/physio_safety_myoElbowPose1D6MRandom-v0.png" alt="Physiological Safety Metrics" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Physiological Safety Metrics</h3>
                            <p class="figure-description">Joint safety constraint violations across episodes. The number of joint angle and joint velocity limit violations is reported per episode. Violations are infrequent and isolated, indicating that the learned policy largely respects physiological and mechanical safety constraints. Joint angle and velocity violations occur only sporadically, suggesting that safety constraints are largely satisfied during execution.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Domain Generalization and Multitask Evaluation -->
            <section id="generalization" class="section">
                <h2 class="section-title">Domain Generalization and Multitask Evaluation</h2>
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/domain_randomization_myoElbowPose1D6MRandom-v0.png" alt="Domain Randomization Results" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Domain Randomization Results</h3>
                            <p class="figure-description">Episode returns under different domain randomization settings. Box plots illustrate the distribution of episode returns across five domain randomization configurations. Performance remains stable across most settings, with increased variability observed only under the most aggressive randomization condition. The policy maintains high performance across a wide range of domain randomization settings, with noticeable degradation only under the strongest randomization.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/multi_task_generalization_myoElbowPose1D6MRandom-v0.png" alt="Multitask Generalization Results" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Multitask Generalization Results</h3>
                            <p class="figure-description">Generalization performance across different environments. Episode return distributions are shown for the randomized and fixed environments. Comparable performance across environments indicates effective generalization beyond the training domain. The learned policy achieves comparable returns across randomized and fixed environments, indicating good generalization capability.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Failure and Control Analysis -->
            <section id="failure" class="section">
                <h2 class="section-title">Failure and Control Analysis</h2>
                <div class="figure-grid">
                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/failure_1d_success_param_myoElbowPose1D6MRandom-v0.png" alt="Failure: Success Rate vs Parameter" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Failure: Success Rate vs Parameter</h3>
                            <p class="figure-description">Task success rate as a function of the control parameter. Success rate exhibits a non-monotonic dependence on the parameter, with peak performance observed at intermediate values. Shaded regions indicate variability across evaluation runs.</p>
                        </div>
                    </div>

                    <div class="figure-card">
                        <div class="figure-image-container">
                            <img src="figures/failure_1d_return_param_myoElbowPose1D6MRandom-v0.png" alt="Failure: Return vs Parameter" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Failure: Return vs Parameter</h3>
                            <p class="figure-description">Mean episode return as a function of the control parameter. Mean return follows a similar non-monotonic trend as success rate, peaking at intermediate parameter values and decreasing toward extreme settings.</p>
                        </div>
                    </div>
                </div>
                
                <!-- Second row: Control Divergence (enlarged) -->
                <div class="figure-grid full-width-card">
                    <div class="figure-card">
                        <div class="figure-image-container enlarged-image">
                            <img src="figures/failure_control_divergence_representative_myoElbowPose1D6MRandom-v0.png" alt="Control Divergence (Representative)" class="figure-image">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Control Divergence (Representative)</h3>
                            <p class="figure-description">Control divergence failure cases. Example trajectories from three failed episodes illustrating control divergence. Left panels show action signals across control dimensions, characterized by high-frequency oscillations and lack of coordination. Right panels show corresponding state observations, where sustained oscillatory control leads to gradual state drift and eventual divergence in critical observation dimensions.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Value Function Analysis -->
            <section id="value" class="section">
                <h2 class="section-title">Value Function Analysis</h2>
                <div class="figure-grid full-width-card">
                    <div class="figure-card">
                        <div class="figure-image-container vertical-image">
                            <img src="figures/value_heatmap_myoElbowPose1D6MRandom-v0.png" alt="Value Heatmap" class="figure-image vertical-figure">
                        </div>
                        <div class="figure-info">
                            <h3 class="figure-title">Value Heatmap</h3>
                            <p class="figure-description">Value function visualization for the first two observation dimensions. The heatmap (top) shows a sharp separation between high- and low-value regions in the state space, while the 3D surface (bottom) reveals a steep value cliff at the boundary. This structure indicates that small state deviations near the boundary can lead to large drops in predicted return, explaining the observed control divergence failures.</p>
                        </div>
                    </div>
                </div>
            </section>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 RL PPO Coursework Results. All figures and descriptions are from the project reports.</p>
        </div>
    </footer>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>